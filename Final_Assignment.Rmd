---
title: "Final Assignment"
author: "vidhi.ambwani@rutgers.edu"
date: "4/17/2023"
output: html_document
---

#PCA
```{r}
library(readr)
Diabetes <- read_csv("C:/Users/vidhi/Downloads/Diabetes.csv")
str(Diabetes)
attach(Diabetes)
#Get the Correlations between the measurements
cor(Diabetes[-1])
# Using prcomp to compute the principal components (eigenvalues and eigenvectors). With scale=TRUE, variable means are set to zero, and variances set to one
Diabetes_pca <- prcomp(Diabetes[,-1],scale=TRUE)
Diabetes_pca
summary(Diabetes_pca)
# sample scores stored in Diabetes_pca$x
# singular values (square roots of eigenvalues) stored in sparrow_pca$sdev
# loadings (eigenvectors) are stored in Diabetes_pca$rotation
# variable means stored in Diabetes_pca$center
# variable standard deviations stored in Diabetes_pca$scale
# A table containing eigenvalues and %'s accounted, follows
# Eigenvalues are sdev^2
(eigen_Diabetes <- Diabetes_pca$sdev^2)
names(eigen_Diabetes) <- paste("PC",1:15,sep="")
eigen_Diabetes
sumlambdas <- sum(eigen_Diabetes)
sumlambdas
propvar <- eigen_Diabetes/sumlambdas
propvar
cumvar_Diabetes <- cumsum(propvar)
cumvar_Diabetes
matlambdas <- rbind(eigen_Diabetes,propvar,cumvar_Diabetes)
rownames(matlambdas) <- c("Eigenvalues","Prop. variance","Cum. prop. variance")
round(matlambdas,4)
summary(Diabetes_pca)
Diabetes_pca$rotation
print(Diabetes_pca)
## Sample scores stored in sparrow_pca$x
Diabetes_pca$x
# Identifying the scores by their survival status
Diabetestyp_pca <- cbind(data.frame(Diabetes_binary),Diabetes_pca$x)
Diabetestyp_pca
# Means of scores for all the PC's classified by Survival status
tabmeansPC <- aggregate(Diabetestyp_pca[,2:6],by=list(Diabetes_binary=Diabetes$Diabetes_binary),mean)
tabmeansPC
tabmeansPC <- tabmeansPC[rev(order(tabmeansPC$Diabetes_binary)),]
tabmeansPC
tabfmeans <- t(tabmeansPC[,-1])
tabfmeans
colnames(tabfmeans) <- t(as.vector(tabmeansPC[1]$Diabetes_binary))
tabfmeans
# Standard deviations of scores for all the PC's classified by Survival status
tabsdsPC <- aggregate(Diabetestyp_pca[,2:16],by=list(Diabetes_binary=Diabetes$Diabetes_binary),sd)
tabfsds <- t(tabsdsPC[,-1])
colnames(tabfsds) <- t(as.vector(tabsdsPC[1]$Diabetes_binary))
tabfsds
t.test(PC1~Diabetes$Diabetes_binary,data=Diabetestyp_pca)
t.test(PC2~Diabetes$Diabetes_binary,data=Diabetestyp_pca)
t.test(PC3~Diabetes$Diabetes_binary,data=Diabetestyp_pca)
t.test(PC4~Diabetes$Diabetes_binary,data=Diabetestyp_pca)
t.test(PC5~Diabetes$Diabetes_binary,data=Diabetestyp_pca)
## F ratio tests
var.test(PC1~Diabetes$Diabetes_binary,data=Diabetestyp_pca)
var.test(PC2~Diabetes$Diabetes_binary,data=Diabetestyp_pca)
var.test(PC3~Diabetes$Diabetes_binary,data=Diabetestyp_pca)
var.test(PC4~Diabetes$Diabetes_binary,data=Diabetestyp_pca)
var.test(PC5~Diabetes$Diabetes_binary,data=Diabetestyp_pca)

# Plotting the scores for the first and second components
plot(Diabetestyp_pca$PC1, Diabetestyp_pca$PC2,pch=ifelse(Diabetestyp_pca$Diabetes_binary == "S",1,16),xlab="PC1", ylab="PC2", main="49 Diabetes against values for PC1 & PC2")
abline(h=0)
abline(v=0)
legend("bottomleft", legend=c("Diabetic","Non-Diabetic"), pch=c(1,16))
plot(eigen_Diabetes, xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")
plot(log(eigen_Diabetes), xlab = "Component number",ylab = "log(Component variance)", type="l",main = "Log(eigenvalue) diagram")
print(summary(Diabetes_pca))
diag(cov(Diabetes_pca$x))
xlim <- range(Diabetes_pca$x[,1])
Diabetes_pca$x[,1]
Diabetes_pca$x
plot(Diabetes_pca$x,xlim=xlim,ylim=xlim)
Diabetes_pca$rotation[,1]
Diabetes_pca$rotation
plot(Diabetes[,-1])
Diabetes_pca$x
plot(Diabetes_pca)
#get the original value of the data based on PCA
center <- Diabetes_pca$center
scale <- Diabetes_pca$scale
new_sparrow <- as.matrix(Diabetes[,-1])
new_sparrow
drop(scale(new_sparrow,center=center, scale=scale)%*%Diabetes_pca$rotation[,1])
predict(Diabetes_pca)[,1]
#The aboved two gives us the same thing. predict is a good function to know.
Diabetes$Diabetes_binary <- as.factor(Diabetes$Diabetes_binary)
out <- sapply(1:15, function(i){plot(Diabetes$Diabetes_binary,Diabetes_pca$x[,i],xlab=paste("PC",i,sep=""),ylab="Diabetes_binary")})
pairs(Diabetes_pca$x[,1:15], ylim = c(-6,4),xlim = c(-6,4),panel=function(x,y,...){text(x,y,Diabetes$Diabetes_binary)})

# Better Ways to Visualize

library(factoextra)
library(FactoMineR)
library(ggfortify)
library(psych)
library(corrplot)
library(devtools)

# Correlation
pairs.panels(Diabetes[,-1],
             gap = 0,
             bg = c("red", "blue")[Diabetes$Diabetes_binary],
             pch=21)

pairs.panels(Diabetes_pca$x,
             gap=0,
             bg = c("red", "blue")[Diabetes$Diabetes_binary],
             pch=21)




fviz_eig(Diabetes_pca, addlabels = TRUE)
fviz_pca_var(Diabetes_pca,col.var = "cos2",
             gradient.cols = c("#FFCC00", "#CC9933", "#660033", "#330033"),
             repel = TRUE)
fviz_pca_ind(Diabetes_pca, col.ind = "cos2", 
                  gradient.cols = c("#FFCC00", "#CC9933", "#660033", "#330033"), 
                  repel = TRUE)
biplot(Diabetes_pca)
autoplot(Diabetes_pca,
         data = Diabetes[,-1],
         loadings = TRUE,
         labels = Diabetes$Diabetes_binary)

# Different PCA Method. 
res.pca <- PCA(Diabetes[,-1], graph = FALSE)
print(res.pca)

# Visualize and Interpret PCA using these functions 

#get_eigenvalue(res.pca): Extract the eigenvalues/variances of principal components
#fviz_eig(res.pca): Visualize the eigenvalues
#get_pca_ind(res.pca), get_pca_var(res.pca): Extract the results for individuals and variables, respectively.
#fviz_pca_ind(res.pca), fviz_pca_var(res.pca): Visualize the results individuals and variables, respectively.
#fviz_pca_biplot(res.pca): Make a biplot of individuals and variables.

eig.val <- get_eigenvalue(res.pca)
eig.val

fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))

var <- get_pca_var(res.pca)
#var$coord: coordinates of variables to create a scatter plot
#var$cos2: represents the quality of representation for variables on the factor map. It’s calculated as the squared coordinates: var.cos2 = var.coord * var.coord.
#var$contrib: contains the contributions (in percentage) of the variables to the principal components. 
#The contribution of a variable (var) to a given principal component is (in percentage) : (var.cos2 * 100) / (total cos2 of the component).
var

# Coordinates
head(var$coord)
# Cos2: quality on the factore map
head(var$cos2)
# Contributions to the principal components
head(var$contrib)

#The plot Below is also known as variable correlation plots. It shows the relationships between all variables. It can be interpreted as follow:

#Positively correlated variables are grouped together.
#Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).
#The distance between variables and the origin measures the quality of the variables on the factor map. 
#Variables that are away from the origin are well represented on the factor map.

# Correlation circle
fviz_pca_var(res.pca, col.var = "black")

# Quality of representation


corrplot(var$cos2, is.corr=FALSE)
# Total cos2 of variables on Dim.1 and Dim.2
#A high cos2 indicates a good representation of the variable on the principal component. 
#In this case the variable is positioned close to the circumference of the correlation circle.
#A low cos2 indicates that the variable is not perfectly represented by the PCs. 
#In this case the variable is close to the center of the circle.

fviz_cos2(res.pca, choice = "var", axes = 1:2)
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )
# Change the transparency by cos2 values
fviz_pca_var(res.pca, alpha.var = "cos2")
corrplot(var$contrib, is.corr=FALSE)
# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
             )
fviz_pca_var(res.pca, alpha.var = "contrib")

fviz_pca_ind(res.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind = Diabetes$Diabetes_binary, # color by groups
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups"
             )


# Description of PC

res.desc <- dimdesc(res.pca, axes = c(1,2,3,4,5), proba = 0.05)
# Description of dimension 1
res.desc$Dim.1
res.desc$Dim.2
res.desc$Dim.3
res.desc$Dim.4
res.desc$Dim.5

# Graph of Indiviuals
ind <- get_pca_ind(res.pca)
ind

## Principal Component Analysis Results for individuals
##  ===================================================
##   Name       Description                       
## 1 "$coord"   "Coordinates for the individuals" 
## 2 "$cos2"    "Cos2 for the individuals"        
## 3 "$contrib" "contributions of the individuals"
#To get access to the different components, use this:

# Coordinates of individuals
head(ind$coord)
# Quality of individuals
head(ind$cos2)
# Contributions of individuals
head(ind$contrib)

fviz_pca_ind(res.pca)

fviz_pca_ind(res.pca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
             )
fviz_pca_ind(res.pca, pointsize = "cos2", 
             pointshape = 21, fill = "#E7B800",
             repel = TRUE # Avoid text overlapping (slow if many points)
             )

fviz_pca_ind(res.pca, col.ind = "cos2", pointsize = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
             )
fviz_cos2(res.pca, choice = "ind")
# Total contribution on PC1 and PC2
fviz_contrib(res.pca, choice = "ind", axes = 1:2)

# Create a random continuous variable of length 23,
# Same length as the number of active individuals in the PCA
set.seed(123)
my.cont.var <- rnorm(49)
# Color individuals by the continuous variable


fviz_pca_ind(res.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind = Diabetes$Diabetes_binary, # color by groups
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups"
             )

fviz_pca_ind(res.pca, geom.ind = "point", col.ind = Diabetes$Diabetes_binary, 
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, ellipse.type = "confidence",
             legend.title = "Groups"
             )
fviz_pca_ind(res.pca,
             label = "none", # hide individual labels
             habillage = Diabetes$Diabetes_binary, # color by groups
             addEllipses = TRUE, # Concentration ellipses
             palette = "jco"
             )
fviz_pca_var(res.pca, geom.var = c("point", "text"))
# Show individuals text labels only
fviz_pca_ind(res.pca, geom.ind =  "text")
# Change the size of arrows an labels
fviz_pca_var(res.pca, arrowsize = 1, labelsize = 5, 
             repel = TRUE)
# Change points size, shape and fill color
# Change labelsize
fviz_pca_ind(res.pca, 
             pointsize = 3, pointshape = 21, fill = "lightblue",
             labelsize = 5, repel = TRUE)

fviz_pca_ind(res.pca,
             geom.ind = "point", # show points only (but not "text")
             group.ind = Diabetes$Diabetes_binary, # color by groups
             legend.title = "Groups",
             mean.point = FALSE)
fviz_pca_ind(res.pca,
             geom.ind = "point", # show points only (but not "text")
             group.ind = Diabetes$Diabetes_binary, # color by groups
             legend.title = "Groups",
             mean.point = TRUE)
fviz_pca_var(res.pca, axes.linetype = "blank")



ind.p <- fviz_pca_ind(res.pca, geom = "point", col.ind = Diabetes$Diabetes_binary)
ggpubr::ggpar(ind.p,
              title = "Principal Component Analysis",
              subtitle = "Diabetes data set",
              caption = "Source: factoextra",
              xlab = "PC1", ylab = "PC2",
              legend.title = "Diabetes_binary", legend.position = "top",
              ggtheme = theme_gray(), palette = "jco"
              )

fviz_pca_biplot(res.pca, repel = TRUE,col.ind = Diabetes$Diabetes_binary,
                col.var = "#2E9FDF", # Variables color
                )

fviz_pca_biplot(res.pca, 
                col.ind = Diabetes$Diabetes_binary, palette = "jco", 
                addEllipses = TRUE, label = "var",
                col.var = "black", repel = TRUE,
                legend.title = "Diabetes_binary") 

fviz_pca_biplot(res.pca, 
                # Fill individuals by groups
                geom.ind = "point",
                pointshape = 21,
                pointsize = 2.5,
                fill.ind = Diabetes$Diabetes_binary,
                col.ind = "black",
                # Color variable by groups
                legend.title = list(fill = "Diabetes_binary", color = "Clusters"),
                repel = TRUE        # Avoid label overplotting
             )+
  ggpubr::fill_palette("jco")+      # Indiviual fill color
  ggpubr::color_palette("npg")      # Variable colors

fviz_pca_biplot(res.pca, 
                # Individuals
                geom.ind = "point",
                fill.ind = Diabetes$Diabetes_binary, col.ind = "black",
                pointshape = 21, pointsize = 2,
                palette = "jco",
                addEllipses = TRUE,
                # Variables
                alpha.var ="contrib", col.var = "contrib",
                gradient.cols = "RdYlBu",
                
                legend.title = list(fill = "Diabetes_binary", color = "Contrib",
                                    alpha = "Contrib")
                )

## http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/
```


# Inference regarding PCA 
* In this particular case, the first PC explains 18.48% of the variance, the second PC explains 8.69%, and so on. Together, the first seven PCs account for 56.08% of the total variability in the data. The remaining PCs contribute less than 5% each to the total variability.
* Factors contributing to 1st dimensionality: General Health,Income, Physical Health, High BP, Education and Physical Activity.
* Factors contributing to 2nd dimensionality: Age and Sex.
* From scree diagram it is visible that 1st 2 dimensions are only important i.e the factors: General Health,Income, Physical Health, High BP, Education, Physical Activity, Age and Sex are the only important factors in determing whether a person is diabetic or not.


#EFA
```{r}
# Factor Analysis

library(psych)


Diabetes <- read.csv("C:/Users/vidhi/Downloads/Diabetes.csv")

attach(Diabetes)
Diabetes[1]
fit.pc <- principal(Diabetes[-1], nfactors=4, rotate="varimax")
fit.pc
round(fit.pc$values, 3)
fit.pc$loadings
# Loadings with more digits
for (i in c(1,3,2,4)) { print(fit.pc$loadings[[1,i]])}
# Communalities
fit.pc$communality
# Rotated factor scores, Notice the columns ordering: RC1, RC3, RC2 and RC4
fit.pc$scores
# Play with FA utilities

fa.parallel(Diabetes[-1]) # See factor recommendation
fa.plot(fit.pc) # See Correlations within Factors
fa.diagram(fit.pc) # Visualize the relationship
vss(Diabetes[-1]) # See Factor recommendations for a simple structure




# Computing Correlation Matrix
corrm.emp <- cor(Diabetes[-1])
corrm.emp
plot(corrm.emp)
Diabetes_pca <- prcomp(Diabetes[-1], scale=TRUE)
summary(Diabetes_pca)
plot(Diabetes_pca)
# A table containing eigenvalues and %'s accounted, follows. Eigenvalues are the sdev^2
(eigen_Diabetes <- round(Diabetes_pca$sdev^2,3))
round(fit.pc$values, 3)
names(eigen_Diabetes) <- paste("PC",1:9,sep="")
eigen_Diabetes
sumlambdas <- sum(eigen_Diabetes)
sumlambdas
propvar <- round(eigen_Diabetes/sumlambdas,2)
propvar
cumvar_Diabetes <- cumsum(propvar)
cumvar_Diabetes
matlambdas <- rbind(eigen_Diabetes,propvar,cumvar_Diabetes)
matlambdas
rownames(matlambdas) <- c("Eigenvalues","Prop. variance","Cum. prop. variance")
rownames(matlambdas)
eigvec.emp <- Diabetes_pca$rotation
print(Diabetes_pca)
# Taking the first four PCs to generate linear combinations for all the variables with four factors
pcafactors.emp <- eigvec.emp[,1:4]
pcafactors.emp
# Multiplying each column of the eigenvector’s matrix by the square-root of the corresponding eigenvalue in order to get the factor loadings
unrot.fact.emp <- sweep(pcafactors.emp,MARGIN=2,Diabetes_pca$sdev[1:4],`*`)
unrot.fact.emp
# Computing communalities
communalities.emp <- rowSums(unrot.fact.emp^2)
communalities.emp
# Performing the varimax rotation. The default in the varimax function is norm=TRUE thus, Kaiser normalization is carried out
rot.fact.emp <- varimax(unrot.fact.emp)
#View(unrot.fact.emp)
rot.fact.emp
# The print method of varimax omits loadings less than abs(0.1). In order to display all the loadings, it is necessary to ask explicitly the contents of the object $loadings
fact.load.emp <- rot.fact.emp$loadings[1:9,1:4]
fact.load.emp
# Computing the rotated factor scores for the 30 European Countries. Notice that signs are reversed for factors F2 (PC2), F3 (PC3) and F4 (PC4)
scale.emp <- scale(Diabetes[-1])
scale.emp

```

#Logistic Regression

```{r}
library(ggplot2)
library(cowplot)
#library(regclass)
library(caret)
library(e1071)
library(pROC)


data <- read.csv("C:/Users/vidhi/Downloads/Diabetes.csv")
data
head(data) # you see data, but no column names
#####


## Exploratory Analysis

length(data$Sex)  # check the length of the sex variable
length(data$Diabetes_binary)  # check the length of the Diabetes_binary variable
xtabs(~ Diabetes_binary + Sex, data=data)
xtabs(~ Diabetes_binary + Age, data=data)
xtabs(~ Diabetes_binary + Smoker, data=data)
xtabs(~ Diabetes_binary + BMI, data=data)
xtabs(~ Diabetes_binary + Stroke, data=data)
xtabs(~ Diabetes_binary + HighBP, data=data)
xtabs(~ Diabetes_binary + HvyAlcoholConsump, data=data)
xtabs(~ Diabetes_binary + PhysHlth, data=data)



logistic_simple <- glm(Diabetes_binary ~ Age, data=data, family="binomial")
summary(logistic_simple)

#  generating full multiple logistic regression 
full_model <- glm(Diabetes_binary ~ Age + Sex + BMI, family = binomial(link = logit)) 
summary(full_model)

# exponentiate the confidence intervals around the log odds for each predictor variable to obtain the odds 
exp(confint(full_model))

## Lastly, let's  see what this logistic regression predicts, given their ages
predicted.data <- data.frame(probability.of.Diabetes_binary=logistic_simple$fitted.values,age=data$Age)
predicted.data
## We can plot the data...

xtabs(~ probability.of.Diabetes_binary + age, data=predicted.data)
logistic <- glm(Diabetes_binary ~ ., data=data, family="binomial")
summary(logistic)
## Now calculate the overall "Pseudo R-squared" and its p-value
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
(ll.null - ll.proposed) / ll.null
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
predicted.data <- data.frame(probability.of.Diabetes_binary=logistic$fitted.values,Diabetes_binary=data$Diabetes_binary)
predicted.data <- predicted.data[order(predicted.data$probability.of.Diabetes_binary, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
## Lastly, we can plot the predicted probabilities for each sample having
## heart disease and color by whether or not they actually had heart disease
ggplot(data=predicted.data, aes(x=rank, y=probability.of.Diabetes_binary)) +
geom_point(aes(color=Diabetes_binary), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of getting Diabetes")

# From Caret
pdata <- predict(logistic,newdata=data,type="response" )
pdata
data$Diabetes_binary
pdataF <- as.factor(ifelse(test=as.numeric(pdata>0.5) == 0, yes="Diabetic", no="Not-Diabetic"))

#From e1071::
#confusionMatrix(pdataF, data$Diabetes_binary)
# From pROC
roc(data$Diabetes_binary,logistic$fitted.values,plot=TRUE)
par(pty = "s")
roc(data$Diabetes_binary,logistic$fitted.values,plot=TRUE)

## NOTE: By default, roc() uses specificity on the x-axis and the values range
## from 1 to 0. This makes the graph look like what we would expect, but the
## x-axis itself might induce a headache. To use 1-specificity (i.e. the
## False Positive Rate) on the x-axis, set "legacy.axes" to TRUE.
roc(data$Diabetes_binary,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE)
roc(data$Diabetes_binary,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage")

roc(data$Diabetes_binary,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4)
roc(data$Diabetes_binary,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4)
## If we want to find out the optimal threshold we can store the
## data used to make the ROC graph in a variable...
roc.info <- roc(data$Diabetes_binary, logistic$fitted.values, legacy.axes=TRUE)
str(roc.info)
## tpp = true positive percentage
## fpp = false positive precentage
roc.df <- data.frame(tpp=roc.info$sensitivities*100, fpp=(1 - roc.info$specificities)*100,thresholds=roc.info$thresholds)
roc.df
head(roc.df) 
## head() will show us the values for the upper right-hand corner of the ROC graph, when the threshold is so low
## (negative infinity) that every single sample is called "obese".
## Thus TPP = 100% and FPP = 100%
tail(roc.df) 
## tail() will show us the values for the lower left-hand corner
## of the ROC graph, when the threshold is so high (infinity)
## that every single sample is called "not obese".
## Thus, TPP = 0% and FPP = 0%
## now let's look at the thresholds between TPP 60% and 80%
roc.df[roc.df$tpp > 60 & roc.df$tpp < 80,]
roc(data$Diabetes_binary,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE)
roc(data$Diabetes_binary,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE, print.auc=TRUE)
roc(data$Diabetes_binary,logistic$fitted.values,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE, print.auc=TRUE, partial.auc=c(100, 90), auc.polygon = TRUE, auc.polygon.col = "#377eb822", print.auc.x=45)
# Lets do two roc plots to understand which model is better
roc(data$Diabetes_binary, logistic_simple$fitted.values, plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, print.auc=TRUE)
# Lets add the other graph
plot.roc(data$Diabetes_binary, logistic$fitted.values, percent=TRUE, col="#4daf4a", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=40)
legend("bottomright", legend=c("Simple", "Non Simple"), col=c("#377eb8", "#4daf4a"), lwd=4) 
```



# Inference regarding Logistic Regression

* Factors that have p<0.05 are good indicators of whether a person will have diabetes or not:
High BP, BMI,General health,Physical health and Age.
* Intercept: The log-odds of Survival when Age=0 is -2.812. For every unit increase in age the log odds of having diabetes will increase by 0.313.
* As p-value is <0.5 for Age component we will reject null hypothesis. It means that age factor affects a person getting diabetes.
* For every increase in unit age the odds of having diabetes are 0.968 times the odds of those with one age unit less.
* The count of predicted cases increases as the probability of diabetes and age group increases, which is not surprising as diabetes is more common in older age groups.
* An AUC with value of 0.86(leaning more towards 1) says that the model is able to differentiate clearly whether person will be diabetic or non-diabetic.



# Scatter plot
```{r}
library(ggplot2)
library(lattice)
library(ggridges)
library(ggvis)
library(ggthemes)
library(cowplot)
library(gapminder)
library(gganimate)
library(dplyr)
library(tidyverse)
library(grid)
library(gridExtra)
library(RColorBrewer)
library(readr)
Diabetes <- read_csv("C:/Users/vidhi/Downloads/Diabetes.csv")

ggplot(Diabetes, aes(x=Smoker,y=PhysActivity))+geom_point(aes(color=Diabetes_binary)) 

ggplot(Diabetes, aes(x=BMI,y=HeartDiseaseorAttack))+geom_point(aes(color=Sex)) 

ggplot(Diabetes, aes(x=HvyAlcoholConsump,y=HeartDiseaseorAttack)) + facet_wrap(~GenHlth) + geom_point(aes(color=Diabetes_binary))


# Histogram

ggplot(Diabetes, aes(HeartDiseaseorAttack))+geom_histogram(fill='pink', color='white',bins=10)
ggplot(Diabetes, aes(GenHlth))+geom_histogram(aes(fill=after_stat(count)))


ggplot(Diabetes, aes(x=HeartDiseaseorAttack, fill=Diabetes_binary)) + geom_bar()+theme_bw()
ggplot(Diabetes, aes(HeartDiseaseorAttack))+ geom_bar(position="stack", fill='lightblue') 
ggplot(Diabetes, aes(Income)) + facet_grid(.~Diabetes_binary) + geom_bar(position="dodge",fill='lightblue')

ggplot(Diabetes, aes(Sex)) + facet_grid(.~Diabetes_binary) + geom_bar(position="dodge")+scale_fill_manual(values=c("blue", "pink"), name="Sex", labels=c("Male", "Female"))

# Regression

ggplot(Diabetes, aes(x=Income,y=Diabetes_binary))+geom_point()+ geom_smooth(method=lm,se= FALSE)
ggplot(Diabetes, aes(x=Age,y=Diabetes_binary)) + geom_point() + stat_smooth()


# Violin plot

ggplot(Diabetes, aes(x=Diabetes_binary,y=Age)) + geom_violin()
ggplot(Diabetes, aes(x=Diabetes_binary,y=Sex)) + geom_violin()
ggplot(Diabetes, aes(x=Diabetes_binary,y=Income)) + geom_violin()
ggplot(Diabetes, aes(x=Diabetes_binary,y=HeartDiseaseorAttack)) + geom_violin()
ggplot(Diabetes, aes(x=Diabetes_binary,y=MentHlth)) + geom_violin()

# box plot

ggplot(Diabetes, aes(x=Age,y=Income)) + geom_boxplot()
ggplot(Diabetes, aes(x=BMI,y=Age)) + geom_boxplot() + coord_flip()

# density plot and ggridges

ggplot(Diabetes, aes(x=BMI)) + geom_density() 
ggplot(Diabetes, aes(x=Age, fill=Diabetes_binary)) + geom_density() 

ggplot(Diabetes, aes(x=Income, fill=Diabetes_binary)) + geom_density() 

ggplot(Diabetes, aes(x=Education, fill=Diabetes_binary)) + geom_density(alpha=0.3, aes(y=after_stat(scaled))) 

ggplot(Diabetes, aes(x=AnyHealthcare)) + geom_density(color='blue') 

 
# hexbin

ggplot(Diabetes, aes(x=Diabetes_binary, y=PhysHlth)) + geom_hex()

```
# Inference regarding Scatter plot

* From the bar chart we can infer probablity of Males having diabetes are higher as compared to females.
* People with higher income have less probablity of suffering from Diabetes as compared to people with more income.
* People having heartattack have less chances of having Diabetes.
